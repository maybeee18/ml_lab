{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Dimensionality-Reduction\" data-toc-modified-id=\"Dimensionality-Reduction-1\">Dimensionality Reduction</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span></li><li><span><a href=\"#What-is-Dimensionality-Reduction?\" data-toc-modified-id=\"What-is-Dimensionality-Reduction?-3\">What is Dimensionality Reduction?</a></span></li><li><span><a href=\"#Why-do-Dimensionality-Reduction?\" data-toc-modified-id=\"Why-do-Dimensionality-Reduction?-4\">Why do Dimensionality Reduction?</a></span></li><li><span><a href=\"#2-types-of-Dimensionality-Reduction\" data-toc-modified-id=\"2-types-of-Dimensionality-Reduction-5\">2 types of Dimensionality Reduction</a></span></li><li><span><a href=\"#3-Strategies-for-Feature-Selection\" data-toc-modified-id=\"3-Strategies-for-Feature-Selection-6\">3 Strategies for Feature Selection</a></span></li><li><span><a href=\"#1)-Filter-Strategy\" data-toc-modified-id=\"1)-Filter-Strategy-7\">1) Filter Strategy</a></span></li><li><span><a href=\"#What-We-Talk-About-When-We-Talk-About-Variance\" data-toc-modified-id=\"What-We-Talk-About-When-We-Talk-About-Variance-8\">What We Talk About When We Talk About Variance</a></span></li><li><span><a href=\"#Why-Remove-Features-With-Low-Variance\" data-toc-modified-id=\"Why-Remove-Features-With-Low-Variance-9\">Why Remove Features With Low Variance</a></span></li><li><span><a href=\"#χ2-(chi-squared)-\" data-toc-modified-id=\"χ2-(chi-squared)--10\">χ2 (chi-squared) </a></span></li><li><span><a href=\"#2)-Wrapper-Strategy\" data-toc-modified-id=\"2)-Wrapper-Strategy-11\">2) Wrapper Strategy</a></span></li><li><span><a href=\"#Recursive-Feature-Elimination\" data-toc-modified-id=\"Recursive-Feature-Elimination-12\">Recursive Feature Elimination</a></span></li><li><span><a href=\"#3)-Embedded-Strategy-\" data-toc-modified-id=\"3)-Embedded-Strategy--13\">3) Embedded Strategy </a></span></li><li><span><a href=\"#Feature-Projection-for-Dimensionality-Reduction\" data-toc-modified-id=\"Feature-Projection-for-Dimensionality-Reduction-14\">Feature Projection for Dimensionality Reduction</a></span></li><li><span><a href=\"#The-Best-Data-Visualizations-are-Feature-Projection-Methods\" data-toc-modified-id=\"The-Best-Data-Visualizations-are-Feature-Projection-Methods-15\">The Best Data Visualizations are Feature Projection Methods</a></span></li><li><span><a href=\"#Takeaways\" data-toc-modified-id=\"Takeaways-16\">Takeaways</a></span></li><li><span><a href=\"#Further-Study\" data-toc-modified-id=\"Further-Study-17\">Further Study</a></span></li><li><span><a href=\"#Discrete-Dimensionality-Reduction\" data-toc-modified-id=\"Discrete-Dimensionality-Reduction-18\">Discrete Dimensionality Reduction</a></span></li><li><span><a href=\"#Dimensionality-Reduction-for-Visualization\" data-toc-modified-id=\"Dimensionality-Reduction-for-Visualization-19\">Dimensionality Reduction for Visualization</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Dimensionality Reduction</h2></center>\n",
    "<br>\n",
    "<center><img src=\"https://imgs.xkcd.com/comics/flatland.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Flatland: A Romance of Many Dimensions (Illustrated) by Edwin Abbott Abbott](http://www.gutenberg.org/ebooks/201) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- Define Dimensionality Reduction in own words.\n",
    "- Explain the difference between Feature Selection and Feature Projection.\n",
    "- Define and provide examples of the 3 Feature Selection strategies:\n",
    "    1. Filter\n",
    "    2. Wrapper\n",
    "    3. Embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What is Dimensionality Reduction?</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "<center>Learn a mapping of feature dimensions <br> that reduces the number of dimensions <br>while keeping important properties.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Why do Dimensionality Reduction?</h2></center>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Express features in a more meaningful way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Combat the curse of dimensionality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Compensate for human's inability to understand high dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>2 types of Dimensionality Reduction</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "1. Feature selection - learn a subset of existing features.\n",
    "<br>\n",
    "<br>\n",
    "1. Feature projection - learn a projection into new space of dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>3 Strategies for Feature Selection</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "1. Filter strategy\n",
    "<br>\n",
    "<br>\n",
    "1. Wrapper strategy\n",
    "<br>\n",
    "<br>\n",
    "1. Embedded strategy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>1) Filter Strategy</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "<center><img src=\"images/filter.png\" width=\"100%\"/></center>\n",
    "\n",
    "Select features based only on general properties <br>(e.g., information gain or correlation).\n",
    "\n",
    "Features are selected regardless of the algorithm / model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# VarianceThreshold?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`VarianceThreshold` is a __filter__ strategy since it does not look at the model information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What We Talk About When We Talk About Variance</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$s^2 = \\frac{1}{N} \\sum_{i=1}^N(x_i - \\bar x)^2$$  \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<center>In Dimension Reduction, variance is short for univariate feature variance. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Why Remove Features With Low Variance</h2></center>\n",
    "\n",
    "As features approach lowers variance, features are less random (i.e., reduced entropy). \n",
    "\n",
    "Most algorithms perform better with a \"reasonable\" amount of variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The most extreme case is a feature with zero variance (i.e., a constant / no entropy).\n",
    "\n",
    "A ML algorithm can __not__ learn to predict a target from a feature with zero variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Larger variance is an issue if it introduces sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# SelectKBest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`SelectKBest` is a __filter__ strategy since it does not look at the model information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "X_new = SelectKBest(chi2, k=2).fit_transform(X, y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>χ2 (chi-squared) </h2></center>\n",
    "<br>\n",
    "\n",
    "$$ χ^2 = \\sum_{i=1}^n \\frac{(O_i - E_i)^2}{E_i} $$  \n",
    "\n",
    "Chi-square test measures dependence between discrete random variables.\n",
    "\n",
    "Can be used to select the features that are the most likely to be independent of class, therefore irrelevant for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/hqdefault.jpg\" width=\"85%\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.29\n"
     ]
    }
   ],
   "source": [
    "# Calculate chi-square to check if a die is loaded\n",
    "observed = [22, 24, 38, 30, 46, 44]\n",
    "expected = [sum(observed) / len(observed)] * len(observed)\n",
    "\n",
    "chi_squared = 0\n",
    "for i, o in enumerate(observed):\n",
    "    chi_squared += (o-expected[i])**2 / expected[i]\n",
    "print(f\"{chi_squared:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://www.youtube.com/watch?v=1Ldl5Zfcm1Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>2) Wrapper Strategy</h2></center>\n",
    "\n",
    "<center><img src=\"images/wrapper.png\" width=\"75%\"/></center>\n",
    "\n",
    "Iteratively evaluate subsets of variables.\n",
    "\n",
    "Examples:\n",
    "\n",
    "- Sequential feature selection algorithms\n",
    "- Recursive feature elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Also most genetic algorithms use the wrapper strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Recursive Feature Elimination</h2></center>\n",
    "\n",
    "The algorithm is fit over and over.\n",
    "\n",
    "Each time the weakest feature(s) are removed until the specified number of features is reached. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# RFECV?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>3) Embedded Strategy </h2></center>\n",
    "<center><img src=\"images/embedded.png\" width=\"65%\"/></center>\n",
    "An algorithm's fitting process can inherently selection features, thus performing feature selection and regression/classification simultaneously.\n",
    "\n",
    "Examples:\n",
    "- Regularized Regression\n",
    "- Tree-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Sources:\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Feature_selection\n",
    "- https://sebastianraschka.com/faq/docs/feature_sele_categories.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Feature Projection for Dimensionality Reduction</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<center>Learn a mapping to a <b>new</b> set of dimensions.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>The Best Data Visualizations are Feature Projection Methods</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "Humans can only visualize in 1-3 dimensions.\n",
    "\n",
    "Interesting data often has __many__ more dimensions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Feature Selection does not work well for data visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Takeaways</h2></center>\n",
    "\n",
    "- Dimensionality Reduction learns a lower number of features by either subsetting (Feature Selection) or learning new dimensions (Feature Projection).\n",
    "- Feature Selection is done by:\n",
    "    + Filter - Remove features based on a model-independent criteria.\n",
    "    + Wrapper - Repeat the model fitting process, based on the results remove features.\n",
    "    + Embedded - In the model fitting processing, remove features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Further Study\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Discrete Dimensionality Reduction</h2></center>\n",
    "\n",
    "[Factor Analysis](https://en.wikipedia.org/wiki/Factor_analysis)\n",
    "\n",
    "\n",
    "Very useful for survey data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Dimensionality Reduction for Visualization\n",
    "------\n",
    "\n",
    "<center><img src=\"https://lvdmaaten.github.io/tsne/examples/caltech101_tsne.jpg\" width=\"25%\"/></center>\n",
    "\n",
    "[Larger version of image](https://lvdmaaten.github.io/tsne/examples/caltech101_tsne.jpg)\n",
    "\n",
    "T-distributed Stochastic Neighbor Embedding (t-SNE) is currently popular.\n",
    "\n",
    "Uses KL divergence!\n",
    "\n",
    "(There are many others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# In scikit-learn\n",
    "\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source:\n",
    "\n",
    "- https://lvdmaaten.github.io/tsne/\n",
    "- Data Reduction Techniques for Scientific Visualization and Data Analysis https://pdfs.semanticscholar.org/e555/db81c878deb01f0e8003bc90a30898e3d518.pdf\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
