{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Hyperparameter-Tuning\" data-toc-modified-id=\"Hyperparameter-Tuning-1\">Hyperparameter Tuning</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span></li><li><span><a href=\"#-Machine-Learning-Training\" data-toc-modified-id=\"-Machine-Learning-Training-3\"> Machine Learning Training</a></span></li><li><span><a href=\"#ML-is-&quot;double-loop&quot;-optimization\" data-toc-modified-id=\"ML-is-&quot;double-loop&quot;-optimization-4\">ML is \"double loop\" optimization</a></span></li><li><span><a href=\"#What-are-parameter-search-methods?\" data-toc-modified-id=\"What-are-parameter-search-methods?-5\">What are parameter search methods?</a></span></li><li><span><a href=\"#1st-Order-Optimization-Review\" data-toc-modified-id=\"1st-Order-Optimization-Review-6\">1<sup>st</sup> Order Optimization Review</a></span></li><li><span><a href=\"#Calculus-Refresher\" data-toc-modified-id=\"Calculus-Refresher-7\">Calculus Refresher</a></span></li><li><span><a href=\"#Loss-Function\" data-toc-modified-id=\"Loss-Function-8\">Loss Function</a></span></li><li><span><a href=\"#1st-Order-Optimization\" data-toc-modified-id=\"1st-Order-Optimization-9\">1st Order Optimization</a></span></li><li><span><a href=\"#2nd-Order-Optimization\" data-toc-modified-id=\"2nd-Order-Optimization-10\">2nd Order Optimization</a></span></li><li><span><a href=\"#1st-vs.-2nd-Order-Optimization\" data-toc-modified-id=\"1st-vs.-2nd-Order-Optimization-11\">1<sup>st</sup> vs. 2<sup>nd</sup> Order Optimization</a></span></li><li><span><a href=\"#1st-vs.-2nd-Order-Optimization\" data-toc-modified-id=\"1st-vs.-2nd-Order-Optimization-12\">1<sup>st</sup> vs. 2<sup>nd</sup> Order Optimization</a></span></li><li><span><a href=\"#2nd-order-Optimizers-Limitations\" data-toc-modified-id=\"2nd-order-Optimizers-Limitations-13\">2nd order Optimizers Limitations</a></span></li><li><span><a href=\"#lbfgs-Solver-for-Logistic-Regression\" data-toc-modified-id=\"lbfgs-Solver-for-Logistic-Regression-14\">lbfgs Solver for Logistic Regression</a></span></li><li><span><a href=\"#lbfgs-Solver-for-Logistic-Regression\" data-toc-modified-id=\"lbfgs-Solver-for-Logistic-Regression-15\">lbfgs Solver for Logistic Regression</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-16\">Check for understanding</a></span></li><li><span><a href=\"#Manual-Search\" data-toc-modified-id=\"Manual-Search-17\">Manual Search</a></span></li><li><span><a href=\"#Life-is-too-short-for-Manual-Search\" data-toc-modified-id=\"Life-is-too-short-for-Manual-Search-18\">Life is too short for Manual Search</a></span></li><li><span><a href=\"#Let's-automate-Manual-Search!\" data-toc-modified-id=\"Let's-automate-Manual-Search!-19\">Let's automate Manual Search!</a></span></li><li><span><a href=\"#Grid-Search\" data-toc-modified-id=\"Grid-Search-20\">Grid Search</a></span></li><li><span><a href=\"#Grid-Search,-aka-just-try-everything!\" data-toc-modified-id=\"Grid-Search,-aka-just-try-everything!-21\">Grid Search, aka just try everything!</a></span></li><li><span><a href=\"#Cartesian-Product\" data-toc-modified-id=\"Cartesian-Product-22\">Cartesian Product</a></span></li><li><span><a href=\"#What-is-the-&quot;Curse-of-Dimensionality&quot;?\" data-toc-modified-id=\"What-is-the-&quot;Curse-of-Dimensionality&quot;?-23\">What is the \"Curse of Dimensionality\"?</a></span></li><li><span><a href=\"#Why-is-&quot;Curse-of-Dimensionality&quot;-an-issue-for-Grid-Search?\" data-toc-modified-id=\"Why-is-&quot;Curse-of-Dimensionality&quot;-an-issue-for-Grid-Search?-24\">Why is \"Curse of Dimensionality\" an issue for Grid Search?</a></span></li><li><span><a href=\"#&quot;Curse-of-dimensionality&quot;-example\" data-toc-modified-id=\"&quot;Curse-of-dimensionality&quot;-example-25\">\"Curse of dimensionality\" example</a></span></li><li><span><a href=\"#Enumerating-Grid-Search\" data-toc-modified-id=\"Enumerating-Grid-Search-26\">Enumerating Grid Search</a></span></li><li><span><a href=\"#Logistic-Regression-Hyperparameter-Serach:-Categorical\" data-toc-modified-id=\"Logistic-Regression-Hyperparameter-Serach:-Categorical-27\">Logistic Regression Hyperparameter Serach: <br>Categorical</a></span></li><li><span><a href=\"#Numeric\" data-toc-modified-id=\"Numeric-28\">Numeric</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-29\">Check for understanding</a></span></li><li><span><a href=\"#Logistic-Regression-Hyperparameter-Serach:-Numeric\" data-toc-modified-id=\"Logistic-Regression-Hyperparameter-Serach:-Numeric-30\">Logistic Regression Hyperparameter Serach: <br>Numeric</a></span></li><li><span><a href=\"#Grid-Search-with-Logistic-Regression-on-Iris-dataset\" data-toc-modified-id=\"Grid-Search-with-Logistic-Regression-on-Iris-dataset-31\">Grid Search with Logistic Regression on Iris dataset</a></span></li><li><span><a href=\"#Grid-vs-Random-Search\" data-toc-modified-id=\"Grid-vs-Random-Search-32\">Grid vs Random Search</a></span></li><li><span><a href=\"#-Random-Search->-Grid-Search:--Both-speed-and-performance\" data-toc-modified-id=\"-Random-Search->-Grid-Search:--Both-speed-and-performance-33\"> Random Search &gt; Grid Search: <br> Both speed and performance</a></span></li><li><span><a href=\"#Takeaways\" data-toc-modified-id=\"Takeaways-34\">Takeaways</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Hyperparameter Tuning</h2></center>\n",
    "\n",
    "<center><img src=\"images/yoel-post-image-4.jpg\" width=\"85%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- Define the \"double loop\" optimization problem in Machine Learning.\n",
    "- Compare and contrast 1st and 2nd order methods for parameter search.\n",
    "- Describe methods for hyperparameter search:\n",
    "    + Manual Search\n",
    "    + Grid Search\n",
    "    + Random Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Machine Learning Training</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "$$Features + Algorithm + Hyperparameters = Model\\ Parameters$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>ML is \"double loop\" optimization</h2></center>\n",
    "\n",
    "<center><img src=\"images/m system simple.png\" width=\"60%\"/></center>\n",
    "\n",
    "<center>Inner loop is model training.</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>Outer loop is hyperparameter tuning.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>Both loops are just search problems.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What are parameter search methods?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Closed form solution (e.g., ordinary least squares (OLS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If possible always choose a closed form solution, a closed form solution will be the fastest and with best guarantees of finding optimal solution.\n",
    "\n",
    "However, it not always possible. OLS is not applicable for very large datasets (large $n$ and/or $p$) or datasets where the inverse of $X^TX$ does not exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- 1<sup>st</sup> order methods (e.g., gradient descent)\n",
    "- 2<sup>nd</sup>order methods (e.g., Newton's method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Manual Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Grid Search\n",
    "- Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Bayesian Optimization\n",
    "\n",
    "Good place to start learning:\n",
    "\n",
    "+ [video](https://www.youtube.com/watch?v=J6UcAdH54RE)\n",
    "+ [slides](https://www.slideshare.net/SigOpt/using-bayesian-optimization-to-tune-machine-learning-models-62205716)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>1<sup>st</sup> Order Optimization Review</h2></center>\n",
    "\n",
    "<center><img src=\"images/1_WKE8FNkYjqJk_qfeuvio6A.png\" width=\"45%\"/></center>\n",
    "\n",
    "- Most common\n",
    "- Efficient on per-iteration basis.\n",
    "- However, overall slow.\n",
    "- No strong guarantees on finding optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Image Source: https://www.sciencemag.org/news/2018/05/ai-researchers-allege-machine-learning-alchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Calculus Refresher</h2></center>\n",
    "\n",
    "<center><img src=\"images/First-order-and-Second-order-derivatives-of-a-function-applied-to-an-edge-shown-by-the.png\" width=\"55%\"/></center>\n",
    "\n",
    "- The function itself.\n",
    "- 1st order derivative - The rate of change of the function.\n",
    "- 2nd order derivative -  The rate of change of the slope of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Loss Function</h2></center>\n",
    "\n",
    "<center><img src=\"images/loss.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>1st Order Optimization</h2></center>\n",
    "\n",
    "<center><img src=\"images/first.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>2nd Order Optimization</h2></center>\n",
    "\n",
    "<center><img src=\"images/second.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>1<sup>st</sup> vs. 2<sup>nd</sup> Order Optimization</h2></center>\n",
    "\n",
    "<center><img src=\"images/vs.png\" width=\"75%\"/></center>\n",
    "\n",
    "2<sup>nd</sup> order optimization is faster to converge because it takes steps based on the rate of curve change.\n",
    "\n",
    "In other words, it is a full step jumps directly to the minimum of the local squared approx."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Gradient Descent is a 1<sup>st</sup> order technique that must take steps along the gradient.\n",
    "\n",
    "Netwon's method is a 2<sup>nd</sup> order technique that can take steps along the gradient of the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>1<sup>st</sup> vs. 2<sup>nd</sup> Order Optimization</h2></center>\n",
    "<br>\n",
    "<center><img src=\"images/hessian.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The Hessian is the matrix of second partial derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>2nd order Optimizers Limitations</h2></center>\n",
    "\n",
    "2nd order optimizers are not as common as 1st order optimizations because of the high cost of computing the second-order information.\n",
    " \n",
    "Inverting the Hessian is $O(n^3)$. Most 2nd order methods often use approximations which are much faster $O(n^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Image Source](https://jermwatt.github.io/machine_learning_refined/notes/4_Second_order_methods/4_4_Newtons.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Learn more about Logistic Regression solvers for scikit-learn](https://stackoverflow.com/questions/38640109/logistic-regression-python-solvers-defintions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Sources:\n",
    "    \n",
    "- https://ipvs.informatik.uni-stuttgart.de/mlr/marc/teaching/13-Optimization/04-secondOrderOpt.pdf\n",
    "- http://www.jmlr.org/papers/volume18/16-491/16-491.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>lbfgs Solver for Logistic Regression</h2></center>\n",
    "\n",
    "<center><img src=\"images/lbfgs.png\" width=\"35%\"/></center>\n",
    "\n",
    "- `lbfgs` means Limited-memory Broyden–Fletcher–Goldfarb–Shanno. \n",
    "- Approximates the second derivative matrix updates. \n",
    "- Stores only a few vectors that represent the approximation implicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><h2>lbfgs Solver for Logistic Regression</h2></center>\n",
    "\n",
    "- Tends to be the best (effective and efficient) for small datasets \n",
    "    - Small n so the loss function can be quickly evaluated.\n",
    "    - Small p because only a few vectors are stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "Why is it difficult to tune hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Typically, it is difficult to find the best hyperparameters directly in terms of the training loss.\n",
    "\n",
    "We care about generalization performanc, via held-out data, thus each need to fit the model in order to the estimate a validation score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Learn more:\n",
    "\n",
    "- https://ipvs.informatik.uni-stuttgart.de/mlr/marc/teaching/13-Optimization/04-secondOrderOpt.pdf\n",
    "- https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/\n",
    "- Random Search for Hyper-Parameter Optimization: http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Manual Search</h2></center>\n",
    "\n",
    "<center><img src=\"images/phd-front.jpg\" width=\"45%\"/></center>\n",
    "\n",
    "- aka, GSD: Graduate Student Descent\n",
    "\n",
    "- 100% manual\n",
    "\n",
    "- Trial & (mostly) Error\n",
    "\n",
    "- The most common approach by amateurs, students and researchers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Life is too short for Manual Search</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><h2>Let's automate Manual Search!</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Grid Search</h2></center>\n",
    "\n",
    "<center><img src=\"images/battleship.jpeg\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Grid Search, aka just try everything!</h2></center>\n",
    "\n",
    "Define a grid:\n",
    "\n",
    "1. Define each hyperparameter as a dimension.\n",
    "1. For each dimension, define the range of possible values.\n",
    "1. Try each combination (the cells of the grid). \n",
    "1. At the end, choose the best combination of parameters measured on cross-validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Cartesian Product</h2></center>\n",
    "\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/Cartesian_Product_qtl1.svg/1200px-Cartesian_Product_qtl1.svg.png\" width=\"45%\"/></center>\n",
    "\n",
    "The product of two sets that generates all possible ordered pairs.\n",
    "\n",
    "If you see Cartesian Product, run away because of computational complexity!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What is the \"Curse of Dimensionality\"?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Generally - As dimensionality increases linearly, the volume of the space increases much faster.\n",
    "\n",
    "If you have fixed data, increasing dimensionality increase sparsity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Why is \"Curse of Dimensionality\" an issue for Grid Search?</h2></center>\n",
    "\n",
    "The number of times you are required to evaluate your model grows exponentially with the number of hyperparameters you want to test.\n",
    "\n",
    "Typically, it is computationally intractable to search all relevant dimensions and values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>\"Curse of dimensionality\" example</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "palette = \"Dark2\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " # of Dims # of Points\n",
      "     1            4\n",
      "     2           16\n",
      "     3           64\n",
      "     4          256\n",
      "     5        1,024\n",
      "     6        4,096\n",
      "     7       16,384\n",
      "     8       65,536\n",
      "     9      262,144\n",
      "    10    1,048,576\n"
     ]
    }
   ],
   "source": [
    "values_for_each_dim = 4 # 2 or 3 or 4\n",
    "n_dims = 10\n",
    "\n",
    "print(f\"{'# of Dims':>10} {'# of Points':>10}\")\n",
    "for n_dimensions in range(1, n_dims+1):\n",
    "    total_values = values_for_each_dim**n_dimensions\n",
    "    print(f\"{n_dimensions:>6} {total_values:>12,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Enumerating Grid Search</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "1. Categorical (Strings or Booleans)\n",
    "1. Numeric (Integers or Floats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Logistic Regression Hyperparameter Serach: <br>Categorical</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create regularization penalty space\n",
    "penalty = ['l1', 'l2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create intercept space\n",
    "fit_intercept = ['True', 'False']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Numeric</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "Select a finite set of \"reasonable\" values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "What happens if you select an \"unreasonable\" set of values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"https://i.imgflip.com/2ni2bl.jpg\" width=\"65%\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8]\n",
      "[2 4 6 8]\n"
     ]
    }
   ],
   "source": [
    "# For integers use either `range` or `np.arrage`\n",
    "pure_python = range(2, 10, 2)\n",
    "numpy = np.arange(start=2, stop=10, step=2)\n",
    "print(list(pure_python))\n",
    "print(numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.5   4.375 5.25  6.125 7.   ]\n",
      "[3.16227766e+03 2.37137371e+04 1.77827941e+05 1.33352143e+06\n",
      " 1.00000000e+07]\n"
     ]
    }
   ],
   "source": [
    "# For non-integers use `np.linspace` or `np.logspace`\n",
    "print(np.linspace(start=3.5, stop=7, num=5))\n",
    "print(np.logspace(start=3.5, stop=7, num=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Logistic Regression Hyperparameter Serach: <br>Numeric</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create regularization hyperparameter space\n",
    "# C is the inverse of regularization strength\n",
    "C = np.logspace(0, 4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C [1.00000000e+00 2.78255940e+00 7.74263683e+00 2.15443469e+01\n",
      " 5.99484250e+01 1.66810054e+02 4.64158883e+02 1.29154967e+03\n",
      " 3.59381366e+03 1.00000000e+04]\n",
      "penalty ['l1', 'l2']\n",
      "fit_intercept ['True', 'False']\n"
     ]
    }
   ],
   "source": [
    "# Put together categorical and numeric\n",
    "hyperparameters = dict(C=C, \n",
    "                       penalty=penalty,\n",
    "                       fit_intercept=fit_intercept)\n",
    "for k, v in hyperparameters.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How many cells / configurations are we testing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "40 (10 x 2 x 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Grid Search with Logistic Regression on Iris dataset</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:    3.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf_grid = GridSearchCV(LogisticRegression(), hyperparameters, cv=5, verbose=1)\n",
    "clf_grid.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test set accuracy - 97.37%\n"
     ]
    }
   ],
   "source": [
    "print(f\"The test set accuracy - {clf_grid.score(X_test, y_test):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should we fit intercept: True\n",
      "Best Penalty: l2\n",
      "Best C: 21.544346900318832\n"
     ]
    }
   ],
   "source": [
    "# View best hyperparameters\n",
    "print('Should we fit intercept:', clf_grid.best_estimator_.get_params()['fit_intercept'])\n",
    "print('Best Penalty:', clf_grid.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', clf_grid.best_estimator_.get_params()['C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Grid vs Random Search</h2></center>\n",
    "<br>\n",
    "<center><img src=\"images/grid_vs_random.png\" width=\"80%\"/></center>\n",
    "\n",
    "- Grid Search have to pick specific values.\n",
    "- Random Search can define a distribution, then sample from distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Random Search > Grid Search: <br> Both speed and performance</h2></center>\n",
    "\n",
    "<center><img src=\"images/random_preformance.png\" width=\"50%\"/></center>\n",
    "\n",
    "<center>Blue line is Grid Search accuracy for 100 trials.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Image sources](http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 13 candidates, totalling 65 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  65 out of  65 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "clf_rand = RandomizedSearchCV(LogisticRegression(), \n",
    "                              hyperparameters, \n",
    "                              cv=5, \n",
    "                              n_iter=13, \n",
    "                              verbose=1)\n",
    "clf_rand.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test set accuracy - 97.37%\n"
     ]
    }
   ],
   "source": [
    "print(f\"The test set accuracy - {clf_rand.score(X_test, y_test):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should we fit intercept: False\n",
      "Best Penalty: l1\n",
      "Best C: 59.94842503189409\n"
     ]
    }
   ],
   "source": [
    "# View best hyperparameters\n",
    "print('Should we fit intercept:', clf_rand.best_estimator_.get_params()['fit_intercept'])\n",
    "print('Best Penalty:', clf_rand.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', clf_rand.best_estimator_.get_params()['C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Takeaways</h2></center>\n",
    "\n",
    "- If possible, use 2nd order optimizations.\n",
    "- Hyperparameter is the outer loop optimization, thus can be a very slow.\n",
    "- Grid search is enumerating all possible hyperparameter options and testing each.\n",
    "- Random search is fixed number of random values for hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: https://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html#sphx-glr-auto-examples-model-selection-plot-randomized-search-py\n",
    "\n",
    "https://chrisalbon.com/machine_learning/model_selection/hyperparameter_tuning_using_grid_search/\n",
    "\n",
    "https://chrisalbon.com/machine_learning/model_selection/hyperparameter_tuning_using_random_search/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
