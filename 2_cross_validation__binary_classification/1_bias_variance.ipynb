{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Bias-&amp;-Variance-in-Machine-Learning-Models\" data-toc-modified-id=\"Bias-&amp;-Variance-in-Machine-Learning-Models-1\">Bias &amp; Variance in Machine Learning Models</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span></li><li><span><a href=\"#What-We-Talk-About-When-We-Talk-About-Bias\" data-toc-modified-id=\"What-We-Talk-About-When-We-Talk-About-Bias-3\">What We Talk About When We Talk About Bias</a></span></li><li><span><a href=\"#From-the-last-class,-what-is-a-model?\" data-toc-modified-id=\"From-the-last-class,-what-is-a-model?-4\">From the last class, what is a model?</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-5\">Check for understanding</a></span></li><li><span><a href=\"#Bias-is-Bad\" data-toc-modified-id=\"Bias-is-Bad-6\">Bias is Bad</a></span></li><li><span><a href=\"#High-Bias-Algorithms\" data-toc-modified-id=\"High-Bias-Algorithms-7\">High Bias Algorithms</a></span></li><li><span><a href=\"#What-is-the-highest-Bias-model-in-Linear-Regression?\" data-toc-modified-id=\"What-is-the-highest-Bias-model-in-Linear-Regression?-8\">What is the highest Bias model in Linear Regression?</a></span></li><li><span><a href=\"#What-is-a-low-Bias-model-in-Linear-Regression?-\n",
    "\" data-toc-modified-id=\"What-is-a-low-Bias-model-in-Linear-Regression?-\n",
    "-9\">What is a low Bias model in Linear Regression?<br> \n",
    "</a></span></li><li><span><a href=\"#What-is-the-one-weird-trick-in-decrease-Bias?\" data-toc-modified-id=\"What-is-the-one-weird-trick-in-decrease-Bias?-10\">What is the one weird trick in decrease Bias?</a></span></li><li><span><a href=\"#Easy-to-add-parameters-Linear-Regression-model\" data-toc-modified-id=\"Easy-to-add-parameters-Linear-Regression-model-11\">Easy to add parameters Linear Regression model</a></span></li><li><span><a href=\"#Increase-model-complexity-by-picking-a-more-flexible-algorithm\" data-toc-modified-id=\"Increase-model-complexity-by-picking-a-more-flexible-algorithm-12\">Increase model complexity by picking a more flexible algorithm</a></span></li><li><span><a href=\"#-Increasing-the-complexity/size-of-the-model,-will-(almost)-always-better-fit-the-training-data-set.\" data-toc-modified-id=\"-Increasing-the-complexity/size-of-the-model,-will-(almost)-always-better-fit-the-training-data-set.-13\"> Increasing the complexity/size of the model, will (almost) always better fit the training data set.</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-14\">Check for understanding</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-15\">Check for understanding</a></span></li><li><span><a href=\"#Any-questions-about-Bias?\" data-toc-modified-id=\"Any-questions-about-Bias?-16\">Any questions about Bias?</a></span></li><li><span><a href=\"#What-is-Variance-in-ML?\" data-toc-modified-id=\"What-is-Variance-in-ML?-17\">What is Variance in ML?</a></span></li><li><span><a href=\"#High-Variance-in-general\" data-toc-modified-id=\"High-Variance-in-general-18\">High Variance in general</a></span></li><li><span><a href=\"#High-Variance-in-Regression:-Polynomials\" data-toc-modified-id=\"High-Variance-in-Regression:-Polynomials-19\">High Variance in Regression: Polynomials</a></span></li><li><span><a href=\"#Increasing-Model-Complexity-Will-Increase-Variance-\" data-toc-modified-id=\"Increasing-Model-Complexity-Will-Increase-Variance--20\">Increasing Model Complexity Will Increase Variance </a></span></li><li><span><a href=\"#Big-Trouble-with-High-Variance\" data-toc-modified-id=\"Big-Trouble-with-High-Variance-21\">Big Trouble with High Variance</a></span></li><li><span><a href=\"#Calculating-Variance--&quot;That-is-an-empirical-question&quot;\" data-toc-modified-id=\"Calculating-Variance--&quot;That-is-an-empirical-question&quot;-22\">Calculating Variance <br> \"That is an empirical question\"</a></span></li><li><span><a href=\"#What-Should-You-Do-If-You-Have-High-Variance?\" data-toc-modified-id=\"What-Should-You-Do-If-You-Have-High-Variance?-23\">What Should You Do If You Have High Variance?</a></span></li><li><span><a href=\"#Data-is-the-World's-Best-Regularizer\" data-toc-modified-id=\"Data-is-the-World's-Best-Regularizer-24\">Data is the World's Best Regularizer</a></span></li><li><span><a href=\"#Machine-Learning-Modeling-Goals\" data-toc-modified-id=\"Machine-Learning-Modeling-Goals-25\">Machine Learning Modeling Goals</a></span></li><li><span><a href=\"#Steps-to-Fit-a-Good-ML-Model\" data-toc-modified-id=\"Steps-to-Fit-a-Good-ML-Model-26\">Steps to Fit a Good ML Model</a></span></li><li><span><a href=\"#What-is-bias-variance-trade-off?\" data-toc-modified-id=\"What-is-bias-variance-trade-off?-27\">What is bias-variance trade-off?</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-28\">Check for understanding</a></span></li><li><span><a href=\"#Takeaways\" data-toc-modified-id=\"Takeaways-29\">Takeaways</a></span></li><li><span><a href=\"#Bonus-Material\" data-toc-modified-id=\"Bonus-Material-30\">Bonus Material</a></span></li><li><span><a href=\"#Bias-variance-trade-off:-A-balancing-act\" data-toc-modified-id=\"Bias-variance-trade-off:-A-balancing-act-31\">Bias-variance trade-off: A balancing act</a></span></li><li><span><a href=\"#Bias-variance-trade-off-in-Regression\" data-toc-modified-id=\"Bias-variance-trade-off-in-Regression-32\">Bias-variance trade-off in Regression</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-33\">Check for understanding</a></span></li><li><span><a href=\"#Check-for-understanding\" data-toc-modified-id=\"Check-for-understanding-34\">Check for understanding</a></span></li><li><span><a href=\"#Empirical-trick:-How-do-actually-use-this-trade-off?\" data-toc-modified-id=\"Empirical-trick:-How-do-actually-use-this-trade-off?-35\">Empirical trick: How do actually use this trade-off?</a></span></li><li><span><a href=\"#Two-Types-of-Errors-on-Training-Datasets\" data-toc-modified-id=\"Two-Types-of-Errors-on-Training-Datasets-36\">Two Types of Errors on Training Datasets</a></span></li><li><span><a href=\"#Where-do-errors-come-from?\" data-toc-modified-id=\"Where-do-errors-come-from?-37\">Where do errors come from?</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Bias & Variance in Machine Learning Models</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/1600/1*xfIbyKKMDmjQF9JFuK2Ykg.png\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- Define Bias and Variance both formally and in your own words.\n",
    "- Identify high and low examples of bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What We Talk About When We Talk About Bias</h2></center>\n",
    "\n",
    "We are only discussing technical Bias in predictive modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__NOT__:\n",
    "\n",
    "- Human / Cognitive  biases\n",
    "- Fairness "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>From the last class, what is a model?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Given a general machine learning algorithm, train a specific architecture (set of hyperparameters) to get a specific set of weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "On your own, answer the following questions:\n",
    "\n",
    "- What is BLUE?\n",
    "- What is the most general formula for bias?\n",
    "- In nontechnical terms, what is bias?\n",
    "- How do we estimate bias?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "BLUE - __B__est __L__inear __U__nbiased __E__stimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$Bias[\\hat f(x)] = E[\\hat f(x) - f(x)]$$\n",
    "\n",
    "Expected difference between predicted and observed for a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The most useful empirical estimate of bias is the error rate on the training dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Bias is Bad</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Bias is how much your model __under-fits__ the training data.\n",
    "\n",
    "The goal of training, aka optimization, is to minimize bias.\n",
    "\n",
    "An algorithm that has a high ability to fit the training data has __low__ bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>High Bias Algorithms</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "<center>Produce simple models that fail to capture meaningful patterns in the data, aka under-fitting the training data.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What is the highest Bias model in Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>Model with just an intercept</center>\n",
    "<center><img src=\"images/intercept.png\" width=\"45%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What is a low Bias model in Linear Regression?<br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>Model with many coefficients</center>\n",
    "<center><img src=\"images/low_bias.png\" width=\"70%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "[Source](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What is the one weird trick in decrease Bias?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Make the model more complex!  \n",
    "\n",
    "1. Add more parameters.\n",
    "1. Pick a more flexible algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Easy to add parameters Linear Regression model</h2></center>\n",
    "\n",
    "$\\hat y = \\beta _0 $   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\hat y = \\beta _0 + \\beta _1x $  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\hat y = \\beta _0 + \\beta _1x  + \\beta _2x$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\hat y = \\beta _0 + \\beta _1x  + \\beta _2x + \\beta _3x \\ldots + \\beta_n x$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Increase model complexity by picking a more flexible algorithm</h2></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Fit a decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Fit a bagged tree (Random Forest™)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Fit a boosted tree (XGBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Fit a deep learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2> Increasing the complexity/size of the model, will (almost) always better fit the training data set.</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "When will increasing the complexity/size of the model __not__ increase fit on the training data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>When your model perfectly fits the training set.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "You have a reasonable algorithm (e.g., Random Forest™) but still have high bias. What should you do with features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Larger set of features\n",
    "- Better features\n",
    "\n",
    "Both will increase your model's ability to fit the training dataset, thus lowering bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Any questions about Bias?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What is Variance in ML?</h2></center>\n",
    "\n",
    "Variance is the amount that an algorithm's model will change if a different dataset is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$Var[\\hat f(x)] = E[\\hat f(x)^2] - E[\\hat f(x)]^2$$\n",
    "\n",
    "Intuitively, how much the algorithm will move around its mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>High Variance in general</h2></center>\n",
    "\n",
    "An algorithm has high variance if small changes in the training data can result in large changes in the estimated model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>High Variance in Regression: Polynomials</h2></center>\n",
    "\n",
    "<center><img src=\"images/polynomial.gif\" width=\"90%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Increasing Model Complexity Will Increase Variance </h2></center>\n",
    "<br>\n",
    "<br>\n",
    "As model complexity increases, the model will better fit the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In other words, a more complex models will have model weights that will closely match the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Big Trouble with High Variance</h2></center>\n",
    "\n",
    "As variance increases, more noise in the training data will be fit by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Modeling noise will not generalize.\n",
    "\n",
    "A model that fits the training data well but does not generalize is called __overfitting__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Calculating Variance <br> \"That is an empirical question\"</h2></center>\n",
    "\n",
    "The best useful empirical estimate of Variance is how much worse you do on the test dataset compared to the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Assuming, you have reasonable amount of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, you can only look at test dataset once. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's create mini-test datasets and call them validation datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is cross-validation (CV)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What Should You Do If You Have High Variance?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Collect more data\n",
    "1. Feature selection\n",
    "1. Dimensionality reduction\n",
    "1. Regularization\n",
    "1. Bagging methods (e.g., Random Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Data is the World's Best Regularizer</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A larger training dataset tends to decrease variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Reducing the chance of overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Increasing the chance of generalization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Machine Learning Modeling Goals</h2></center>\n",
    "\n",
    "1) Low bias (does a good job modeling the patterns in the observed data). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) Low variance (does a good job on generalizing to unseen data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3) Profit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "4) Everything else (interpretability, …)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Steps to Fit a Good ML Model</h2></center>\n",
    "\n",
    "1. Collect a lot of high quality data.\n",
    "1. Engineer good features.\n",
    "1. Pick a complex algorithm.\n",
    "1. Train specific models to perform well on the validation dataset.\n",
    "\n",
    "The result is minimal bias __and__ minimal variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What is bias-variance trade-off?</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "<center>Covered in MSDS 621</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "Discuss with a neighbor:\n",
    "\n",
    "How does increasing regularization change Bias {lower, no impact, increase}? Why?\n",
    "\n",
    "How does increasing regularization change Variance {lower, no impact, increase}? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Regularization __almost always increases__ bias. The same algorithm with regularization typically will not fit the training data as well as the same algorithm without regularization.\n",
    "\n",
    "In other words - as regularization goes up, performance on the training dataset will most likely go down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Regularization __might decrease__ variance. The goal of regularization is to increase generalization but there is no guarantees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Takeaways</h2></center>\n",
    "\n",
    "- Bias is the error rate of your model on the training dataset.\n",
    "- Variance is amount of change in your model's performance between training dataset and other datasets.\n",
    "- We want low bias (fit the trianing set) and low variance (fit other datasets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bonus Material\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bias-variance trade-off: A balancing act\n",
    "------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://i.stack.imgur.com/alkeM.png\" width=\"75%\"/></center>\n",
    "\n",
    "The problem of simultaneously minimizing two sources of error that prevent supervised learning algorithms from generalizing beyond their training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"http://www.luigifreda.com/wp-content/uploads/2017/03/Bias-Variance-Tradeoff-660x445.png\" width=\"80%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://pseudotitle.files.wordpress.com/2016/12/bias-variance-decomposition.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bias-variance trade-off in Regression\n",
    "------\n",
    "\n",
    "<center><img src=\"https://codesachin.files.wordpress.com/2015/08/plot_bias_variance_examples_31.png\" width=\"65%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Check for understanding</h2></center>\n",
    "\n",
    "An algorithm that is strongly influenced by the specifics of the training data has \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_ variance.   \n",
    "{high, low} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An algorithm that is strongly influenced by the specifics of the training data __high__ variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example, decision trees tend to change their predicted values for training sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "What does this curve imply? High Variance, High Bias\n",
    "\n",
    "<center><img src=\"images/high_bias.png\" width=\"75%\"/></center>\n",
    "\n",
    "Think, Pair, Share"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/high_bias.png\" width=\"75%\"/></center>\n",
    "\n",
    "High Bias: Your training error is way higher than desired performance!   \n",
    "Low Variance: Small gap between training and test error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Empirical trick: How do actually use this trade-off?\n",
    "------\n",
    "\n",
    "\n",
    "You want to fit a model that does really well on training set (low bias) and well on unseen data (low variance).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Two Types of Errors on Training Datasets</h2></center>\n",
    "\n",
    "$$E(Y - \\hat Y)^2 = E[ f(x) + ε - f(x)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Where do errors come from?</h2></center>\n",
    "\n",
    "<center><img src=\"images/error_types.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
