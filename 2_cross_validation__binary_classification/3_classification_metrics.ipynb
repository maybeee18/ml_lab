{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Classification-Evaluation-Metrics\" data-toc-modified-id=\"Classification-Evaluation-Metrics-1\">Classification Evaluation Metrics</a></span></li><li><span><a href=\"#Learning-Outcomes\" data-toc-modified-id=\"Learning-Outcomes-2\">Learning Outcomes</a></span></li><li><span><a href=\"#Why-should-we-care-about-evaluation-metrics?\" data-toc-modified-id=\"Why-should-we-care-about-evaluation-metrics?-3\">Why should we care about evaluation metrics?</a></span></li><li><span><a href=\"#What-are-common-evaluation-metrics-for-classification?\" data-toc-modified-id=\"What-are-common-evaluation-metrics-for-classification?-4\">What are common evaluation metrics for classification?</a></span></li><li><span><a href=\"#Accuracy\" data-toc-modified-id=\"Accuracy-5\">Accuracy</a></span></li><li><span><a href=\"#What-are-the-biggest-limitations-of-accuracy?\" data-toc-modified-id=\"What-are-the-biggest-limitations-of-accuracy?-6\">What are the biggest limitations of accuracy?</a></span></li><li><span><a href=\"#Null-Accuracy\" data-toc-modified-id=\"Null-Accuracy-7\">Null Accuracy</a></span></li><li><span><a href=\"#Student-Activity\" data-toc-modified-id=\"Student-Activity-8\">Student Activity</a></span></li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-9\">Confusion Matrix</a></span></li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-10\">Confusion Matrix</a></span></li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-11\">Confusion Matrix</a></span></li><li><span><a href=\"#Precision-vs.-Recall\" data-toc-modified-id=\"Precision-vs.-Recall-12\">Precision vs. Recall</a></span></li><li><span><a href=\"#Hot-New-Contest---Dog-or-Bear?\" data-toc-modified-id=\"Hot-New-Contest---Dog-or-Bear?-13\">Hot New Contest - Dog or Bear?</a></span></li><li><span><a href=\"#New-Contest---Dog-or-Bear?\" data-toc-modified-id=\"New-Contest---Dog-or-Bear?-14\">New Contest - Dog or Bear?</a></span></li><li><span><a href=\"#Recall\" data-toc-modified-id=\"Recall-15\">Recall</a></span></li><li><span><a href=\"#Student-Activity:-Calculate-Performance-Metrics-on-Classification\" data-toc-modified-id=\"Student-Activity:-Calculate-Performance-Metrics-on-Classification-16\">Student Activity: Calculate Performance Metrics on Classification</a></span></li><li><span><a href=\"#Recall\" data-toc-modified-id=\"Recall-17\">Recall</a></span></li><li><span><a href=\"#Recall-is-typically-calculated-by-class\" data-toc-modified-id=\"Recall-is-typically-calculated-by-class-18\">Recall is typically calculated by class</a></span></li><li><span><a href=\"#Precision\" data-toc-modified-id=\"Precision-19\">Precision</a></span></li><li><span><a href=\"#Classification---Getting-water-out-of-a-well-with-a-bucket\" data-toc-modified-id=\"Classification---Getting-water-out-of-a-well-with-a-bucket-20\">Classification - Getting water out of a well with a bucket</a></span></li><li><span><a href=\"#F1-score\" data-toc-modified-id=\"F1-score-21\">F<sub>1</sub> score</a></span></li><li><span><a href=\"#Generalized-F-score\" data-toc-modified-id=\"Generalized-F-score-22\">Generalized F score</a></span></li><li><span><a href=\"#Which-metrics-should-you-choose?\" data-toc-modified-id=\"Which-metrics-should-you-choose?-23\">Which metrics should you choose?</a></span></li><li><span><a href=\"#airbnb-Custom-Pricing-Case-Study\" data-toc-modified-id=\"airbnb-Custom-Pricing-Case-Study-24\">airbnb Custom Pricing Case Study</a></span></li><li><span><a href=\"#Takeaways---Part-I\" data-toc-modified-id=\"Takeaways---Part-I-25\">Takeaways - Part I</a></span></li><li><span><a href=\"#Takeaways---Part-II\" data-toc-modified-id=\"Takeaways---Part-II-26\">Takeaways - Part II</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Classification Evaluation Metrics</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "<center><img src=\"https://i.pinimg.com/736x/18/c0/36/18c036f262ef322194553462279e5bbf.jpg\" width=\"100%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Learning Outcomes</h2></center>\n",
    "\n",
    "__By the end of this session, you should be able to__:\n",
    "\n",
    "- List common classification metrics.\n",
    "- Explain the limitations of accuracy as a metric.\n",
    "- Construct a confusion matrix.\n",
    "- Define precision, recall, and F score.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Why should we care about evaluation metrics?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Evaluation metrics help select better, aka more useful, models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which algorithm should we choose?  \n",
    "Which hyperparameters are better?   \n",
    "Are these good parameter estimates?     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What are common evaluation metrics for classification?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Accuracy \n",
    "- Recall\n",
    "- Precision\n",
    "- F-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Accuracy</h2></center>\n",
    "\n",
    "$$Accuracy = \\frac{All\\ Correct}{Total}$$\n",
    "\n",
    "- Fraction of observations classified correctly\n",
    "- 1 - error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>What are the biggest limitations of accuracy?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1) Accuracy is an overall measure. It does not tell you what kind of errors the classifier is making.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) Accuracy is effected by class imbalances (more examples from one category than other categories).\n",
    "\n",
    "Accuracy paradox - A predictive model may have high accuracy but is useless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/accuracy_pie_chart.png\" width=\"110%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Null Accuracy</h2></center>\n",
    "\n",
    "Accuracy that could be achieved by always predicting the most frequent class.\n",
    "\n",
    "Also called baseline accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# What is the null accuracy in this case?\n",
    "n = 10_000\n",
    "n_not_fraud = 9_952\n",
    "n_fraud = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always predicting 'not fraud' will result in 99.52% accuracy.\n"
     ]
    }
   ],
   "source": [
    "null_accuracy = (n - n_fraud)/n\n",
    "print(f\"Always predicting 'not fraud' will result in {null_accuracy:.2%} accuracy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A null accuracy model with imbalanced data is correct almost all of the time but useless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Student Activity</h2></center>\n",
    "\n",
    "<center>Complete this confusion matrix:</center>\n",
    "\n",
    "<center><img src=\"images/confusion_matrix_blank.png\" width=\"45%\"/></center>\n",
    "<center>{True Negatives, True Positives, False Negatives, False Positives}</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Confusion Matrix</h2></center>\n",
    "\n",
    "<center><img src=\"images/confusion_matrix.png\" width=\"40%\"/></center>\n",
    "\n",
    "- `Positive` and `Negative` refer to the predicted result - presence or absence.\n",
    "- `True` and `False` refer to whether the model was correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Confusion Matrix</h2></center>\n",
    "\n",
    "<center><img src=\"images/confusion_matrix.png\" width=\"35%\"/></center>\n",
    "\n",
    "True Positives (TP): correctly predicted a successful outcome. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "True Negatives (TN): correctly predicted a lack of an outcome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Confusion Matrix</h2></center>\n",
    "\n",
    "<center><img src=\"images/confusion_matrix.png\" width=\"35%\"/></center>\n",
    "\n",
    "False Positives (FP): incorrectly predicted a successful outcome (a \"Type I error\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "False Negatives (FN): incorrectly predicted lack of an outcome (a \"Type II error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/pregnant.jpg\" width=\"90%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Precision vs. Recall</h2></center>\n",
    "\n",
    "<center><img src=\"images/confus1.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The same numerator for both precision and recall.\n",
    "\n",
    "The denominator only has a minor difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Hot New Contest - Dog or Bear?</h2></center>\n",
    "\n",
    "<center><img src=\"images/bear vs dog.jpg\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>New Contest - Dog or Bear?</h2></center>\n",
    "\n",
    "<center><img src=\"images/puppy.jpeg\" width=\"45%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "It is dog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Recall</h2></center>\n",
    "\n",
    "$$Recall = \\frac{True\\ Positives}{True\\ Positives + False\\ Negatives} = \\frac{Class\\ Correct}{Class\\ Total\\ Actual}$$\n",
    "<br>\n",
    "<center>Fraction of actual labeled items that are classified correctly.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>Maximize recall to be more certain to find all positive examples.</center>\n",
    "<center>All things that are actually dogs are labeled by the classifier as dogs.</center>\n",
    "<center>We want a classifier to find (recall) positive examples.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Student Activity: Calculate Performance Metrics on Classification</h2></center>\n",
    "\n",
    "<center><img src=\"images/results.png\" width=\"85%\"/></center>\n",
    "\n",
    "The data are the actual labels.\n",
    "\n",
    "Let's compare model 1 vs model 2 predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/data_labels.png\" width=\"85%\"/></center>\n",
    "\n",
    "What is the base-rate for bear / red?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The base-rate for is 20% (2 bear / red out of 10 total).   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/data_labels.png\" width=\"85%\"/></center>\n",
    "\n",
    "__Support__ is the number of examples by class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2 bear / red support examples.  \n",
    "8 dog / blue support examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/model_1.png\" width=\"85%\"/></center>\n",
    "\n",
    "What is the accuracy of Model 1? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "80% accurate -  80% (always predict dog / blue).\n",
    "\n",
    "Model 1 predicts all dog / blue and gets all the dog / blue labels correct. Always missing the 2 bear / red labels.\n",
    "\n",
    "Model 1 is the null accuracy model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/model_2.png\" width=\"85%\"/></center>\n",
    "\n",
    "What is the accuracy of Model 2? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "70% accurate  (2 correct bear / red + 5 correct dog / blue) / 10 total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/results.png\" width=\"85%\"/></center>\n",
    "\n",
    "Model 1 is more accurate than Model 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Recall</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/model_1.png\" width=\"85%\"/></center>\n",
    "\n",
    "What is the recall of Model 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Depends!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Recall is typically calculated by class</h2></center>\n",
    "\n",
    "0% recall for bear / red. The model fails to label any true bear / red as bear / red.     \n",
    "100% recall for dog / blue. The model labels all true dog / blue as dog / blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "y_true = [0, 1, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_pred_model_1 = [0] * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scikit-learn's default is by postive class\n",
    "recall_score(y_true, y_pred_model_1) # Recall for bear / red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_true, y_pred_model_1, pos_label=0) # Recall for dog / blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'macro': Calculate metrics for each label, and find their unweighted mean. \n",
    "# This does not take label imbalance into account.\n",
    "recall_score(y_true, y_pred_model_1, average='macro')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'weighted': Calculate metrics for each label, and \n",
    "# find their average weighted by support \n",
    "# (the number of true instances for each label). \n",
    "recall_score(y_true, y_pred_model_1, average='weighted')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/model_2.png\" width=\"85%\"/></center>\n",
    "\n",
    "What is the recall for red / bear in Model 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "100% recall for red. The model correctly labels all true red as red (2 out 2).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true         = [0, 1, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_pred_model_2 = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "\n",
    "recall_score(y_true, y_pred_model_2) # Recall for bear / red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/model_2.png\" width=\"85%\"/></center>\n",
    "\n",
    "What is the recall for dog / blue for Model 2? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "62.5% recall for dog / blue. The model correctly labels 5 out 8 possible true dog / blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_true, y_pred_model_2, pos_label=0) # Recall for dog / blue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/model_2.png\" width=\"85%\"/></center>\n",
    "\n",
    "What is the weighted recall for Model 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true         = [0, 1, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_pred_model_2 = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "\n",
    "# 'weighted': Calculate metrics for each label, \n",
    "# and find their average weighted by support \n",
    "# (the number of true instances for each label). \n",
    "recall_score(y_true, y_pred_model_2, average='weighted')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Weighted recall is the same as accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Precision</h2></center>\n",
    "\n",
    "$$Precision = \\frac{True\\ Positives}{True\\ Positives + False\\ Positives} = \\frac{Class\\ Correct}{Class\\ Total\\ Predicted}$$\n",
    "<br>\n",
    "<center>Fraction of predicted labeled items assigned to a class that are actually members of that class.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>Maximize precision to be more certain everything labeled as positive is positive.</center>\n",
    "<center>All things predicted as dogs are actually dogs.</center>\n",
    "<center>We want a classifier to <b>precisely predict</b> labels for positives items.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Classification - Getting water out of a well with a bucket</h2></center>\n",
    "<center><img src=\"images/bucket.png\" width=\"30%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall - You get all the water out of well (water is positive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Precision - You have only water in the bucket (no mud / negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/model_1.png\" width=\"85%\"/></center>\n",
    "What is the precision of Model 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "0 out of 10 for bear / red.\n",
    "\n",
    "8 out of 10 for dog / blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 precision for red / bear.\n",
      "0.8 precision for blue / dog.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "print(f\"{precision_score(y_true, y_pred_model_1)} precision for red / bear.\")\n",
    "print(f\"{precision_score(y_true, y_pred_model_1, pos_label=0)} precision for blue / dog.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'weighted': Calculate metrics for each label, and \n",
    "# find their average weighted by support \n",
    "# (the number of true instances for each label). \n",
    "precision_score(y_true, y_pred_model_1, average='weighted')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/model_2.png\" width=\"85%\"/></center>\n",
    "What is the precision of Model 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4 (2 out of 5) precision for red / bear.\n",
      "1.0 (5 out of 5) precision for blue / dog.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{precision_score(y_true, y_pred_model_2)} (2 out of 5) precision for red / bear.\")\n",
    "print(f\"{precision_score(y_true, y_pred_model_2, pos_label=0)} (5 out of 5) precision for blue / dog.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8800000000000001"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average weighted by supportfor each label\n",
    "precision_score(y_true, y_pred_model_2, average='weighted')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>F<sub>1</sub> score</h2></center>\n",
    "\n",
    "$$ F_1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision+Recall} $$\n",
    "\n",
    "A single metric that combines precision and recall.\n",
    "    \n",
    "In Machine Learning, we want a single North Star metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/model_1.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7111111111111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/anaconda3/envs/ml/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(f1_score(y_true, y_pred_model_1, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/model_2.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7296703296703297\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(y_true, y_pred_model_2, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/results.png\" width=\"85%\"/></center>\n",
    "\n",
    "Which model would you deploy into production?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which model performs better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/model_1.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89         8\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.80        10\n",
      "   macro avg       0.40      0.50      0.44        10\n",
      "weighted avg       0.64      0.80      0.71        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred_model_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/model_2.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.62      0.77         8\n",
      "           1       0.40      1.00      0.57         2\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.70      0.81      0.67        10\n",
      "weighted avg       0.88      0.70      0.73        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred_model_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Which model is faster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Model 1 is much faster example of speed / performance trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Generalized F score</h2></center>\n",
    "\n",
    "<center><img src=\"images/f_score_2.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$F_1$ weighs recall and precision equally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$F_{β<1}$ weighs recall lower than precision (by reducing the influence of false negatives). $F_{0.5}$  is a common choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$F_{β>1}$ weighs recall higher than precision (by placing more emphasis on false negatives). $F_{2.0}$  is a common choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"http://www.bluemontlabs.com/images/statistical-classification-metrics.png\" width=\"85%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Which metrics should you choose?</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>Choice of metric depends on your business goals.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>You should define custom evaluation metrics.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>airbnb Custom Pricing Case Study</h2></center>\n",
    "<br>\n",
    "<br>\n",
    "<center><img src=\"images/table.png\" width=\"80%\"/></center>\n",
    "\n",
    "<center>Pricing Sugg(ested) threshold by actual Price</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/custom.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Source: \n",
    "\n",
    "- https://www.kdd.org/kdd2018/accepted-papers/view/customized-regression-model-for-airbnb-dynamic-pricing\n",
    "- https://blog.acolyer.org/2018/10/03/customized-regression-model-for-airbnb-dynamic-pricing/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Takeaways - Part I</h2></center>\n",
    "\n",
    "- The most common classification metrics:\n",
    "    - Accuracy\n",
    "    - Precision\n",
    "    - Recall\n",
    "    - F-score\n",
    "- Accuracy is not accurate if there are class imbalances. In real DS, there are almost always class imbalances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Takeaways - Part II</h2></center>\n",
    "\n",
    "- A confusion matrix is an awesome way to visualize error types.\n",
    "- Brian ♥️s confusion matrices so always bring him one when asking for advice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Precision measures if a classifier has right labels.\n",
    "- Recall measures if a classifier does not miss a label.\n",
    "- F score combines precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
